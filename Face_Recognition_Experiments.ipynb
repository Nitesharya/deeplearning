{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorflow"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CauA2SI2s-lQ",
        "outputId": "51e0e0e1-8a98-4e54-d209-e7727ef1a487"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tensorflow\n",
            "  Downloading tensorflow-2.20.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.5 kB)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (1.4.0)\n",
            "Collecting astunparse>=1.6.0 (from tensorflow)\n",
            "  Downloading astunparse-1.6.3-py2.py3-none-any.whl.metadata (4.4 kB)\n",
            "Collecting flatbuffers>=24.3.25 (from tensorflow)\n",
            "  Downloading flatbuffers-25.9.23-py2.py3-none-any.whl.metadata (875 bytes)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (0.6.0)\n",
            "Collecting google_pasta>=0.1.1 (from tensorflow)\n",
            "  Downloading google_pasta-0.2.0-py3-none-any.whl.metadata (814 bytes)\n",
            "Collecting libclang>=13.0.0 (from tensorflow)\n",
            "  Downloading libclang-18.1.1-py2.py3-none-manylinux2010_x86_64.whl.metadata (5.2 kB)\n",
            "Requirement already satisfied: opt_einsum>=2.3.2 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from tensorflow) (25.0)\n",
            "Requirement already satisfied: protobuf>=5.28.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (6.33.0)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (2.32.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from tensorflow) (75.2.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (1.17.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (3.2.0)\n",
            "Requirement already satisfied: typing_extensions>=3.6.6 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (4.15.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (2.0.1)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (1.76.0)\n",
            "Collecting tensorboard~=2.20.0 (from tensorflow)\n",
            "  Downloading tensorboard-2.20.0-py3-none-any.whl.metadata (1.8 kB)\n",
            "Requirement already satisfied: keras>=3.10.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (3.10.0)\n",
            "Requirement already satisfied: numpy>=1.26.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (2.0.2)\n",
            "Requirement already satisfied: h5py>=3.11.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (3.15.1)\n",
            "Requirement already satisfied: ml_dtypes<1.0.0,>=0.5.1 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (0.5.3)\n",
            "Collecting wheel<1.0,>=0.23.0 (from astunparse>=1.6.0->tensorflow)\n",
            "  Downloading wheel-0.45.1-py3-none-any.whl.metadata (2.3 kB)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.12/dist-packages (from keras>=3.10.0->tensorflow) (14.2.0)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.12/dist-packages (from keras>=3.10.0->tensorflow) (0.1.0)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.12/dist-packages (from keras>=3.10.0->tensorflow) (0.17.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.21.0->tensorflow) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.21.0->tensorflow) (2025.10.5)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/lib/python3/dist-packages (from tensorboard~=2.20.0->tensorflow) (3.3.6)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.12/dist-packages (from tensorboard~=2.20.0->tensorflow) (12.0.0)\n",
            "Collecting tensorboard-data-server<0.8.0,>=0.7.0 (from tensorboard~=2.20.0->tensorflow)\n",
            "  Downloading tensorboard_data_server-0.7.2-py3-none-manylinux_2_31_x86_64.whl.metadata (1.1 kB)\n",
            "Collecting werkzeug>=1.0.1 (from tensorboard~=2.20.0->tensorflow)\n",
            "  Downloading werkzeug-3.1.3-py3-none-any.whl.metadata (3.7 kB)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.12/dist-packages (from werkzeug>=1.0.1->tensorboard~=2.20.0->tensorflow) (3.0.3)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich->keras>=3.10.0->tensorflow) (4.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich->keras>=3.10.0->tensorflow) (2.19.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.10.0->tensorflow) (0.1.2)\n",
            "Downloading tensorflow-2.20.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (620.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m620.7/620.7 MB\u001b[0m \u001b[31m805.8 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
            "Downloading flatbuffers-25.9.23-py2.py3-none-any.whl (30 kB)\n",
            "Downloading google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.5/57.5 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading libclang-18.1.1-py2.py3-none-manylinux2010_x86_64.whl (24.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.5/24.5 MB\u001b[0m \u001b[31m23.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tensorboard-2.20.0-py3-none-any.whl (5.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m133.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tensorboard_data_server-0.7.2-py3-none-manylinux_2_31_x86_64.whl (6.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.6/6.6 MB\u001b[0m \u001b[31m87.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading werkzeug-3.1.3-py3-none-any.whl (224 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m224.5/224.5 kB\u001b[0m \u001b[31m13.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading wheel-0.45.1-py3-none-any.whl (72 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.5/72.5 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: libclang, flatbuffers, wheel, werkzeug, tensorboard-data-server, google_pasta, tensorboard, astunparse, tensorflow\n",
            "Successfully installed astunparse-1.6.3 flatbuffers-25.9.23 google_pasta-0.2.0 libclang-18.1.1 tensorboard-2.20.0 tensorboard-data-server-0.7.2 tensorflow-2.20.0 werkzeug-3.1.3 wheel-0.45.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RKjKnWbFs_TK",
        "outputId": "0cf3140a-a61f-4363-a122-7b8a122ba450"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "import os\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-GwsJ3SUtLiD",
        "outputId": "e326f1b3-d1a5-462f-9206-94d1320c9510"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/jax/_src/cloud_tpu_init.py:86: UserWarning: Transparent hugepages are not enabled. TPU runtime startup and shutdown time should be significantly improved on TPU v5e and newer. If not already set, you may need to enable transparent hugepages in your VM image (sudo sh -c \"echo always > /sys/kernel/mm/transparent_hugepage/enabled\")\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "DATA_PATH = '/content/drive/MyDrive/video_face_recognition(nitesh)/3_croped_images'\n",
        "BATCH_SIZE = 32\n",
        "EPOCHS = 10\n",
        "\n",
        "print(\"Step 1 Complete: Libraries loaded.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lXtJEdT9tBbE",
        "outputId": "ad725cf7-5bb5-4cf8-8f0a-bd17a8eb760d"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 1 Complete: Libraries loaded.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def load_data(img_size):\n",
        "    print(f\"\\n--- Loading Data at size {img_size}x{img_size} ---\")\n",
        "\n",
        "    try:\n",
        "        # 1. Load Training Data (80% of images)\n",
        "        train_ds = tf.keras.utils.image_dataset_from_directory(\n",
        "            DATA_PATH,\n",
        "            validation_split=0.2,\n",
        "            subset=\"training\",\n",
        "            seed=123,\n",
        "            image_size=(img_size, img_size),\n",
        "            batch_size=BATCH_SIZE\n",
        "        )\n",
        "\n",
        "        # 2. Load Validation Data (20% of images)\n",
        "        val_ds = tf.keras.utils.image_dataset_from_directory(\n",
        "            DATA_PATH,\n",
        "            validation_split=0.2,\n",
        "            subset=\"validation\",\n",
        "            seed=123,\n",
        "            image_size=(img_size, img_size),\n",
        "            batch_size=BATCH_SIZE\n",
        "        )\n",
        "\n",
        "        # 3. Normalize Colors (Change pixel values from 0-255 to 0-1)\n",
        "        normalization_layer = layers.Rescaling(1./255)\n",
        "        train_ds = train_ds.map(lambda x, y: (normalization_layer(x), y))\n",
        "        val_ds = val_ds.map(lambda x, y: (normalization_layer(x), y))\n",
        "\n",
        "        # 4. Performance Optimization (Pre-loading data)\n",
        "        train_ds = train_ds.cache().prefetch(buffer_size=tf.data.AUTOTUNE)\n",
        "        val_ds = val_ds.cache().prefetch(buffer_size=tf.data.AUTOTUNE)\n",
        "\n",
        "        return train_ds, val_ds\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error: {e}\")\n",
        "        return None, None\n",
        "\n",
        "print(\"Step 2 Complete: Data Loader defined.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UFGwSwpQt4K-",
        "outputId": "482b1e0a-a432-4598-9283-af84fe609ba8"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 2 Complete: Data Loader defined.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def build_cnn_model(input_shape, num_conv_layers, use_dropout, num_classes):\n",
        "    model = models.Sequential()\n",
        "\n",
        "    # 1. Start with input\n",
        "    model.add(layers.InputLayer(input_shape=input_shape))\n",
        "\n",
        "    # 2. Add Convolutional Layers dynamically\n",
        "    for i in range(num_conv_layers):\n",
        "        # Filter size increases: 32 -> 64 -> 128\n",
        "        num_filters = 32 * (2**i)\n",
        "        model.add(layers.Conv2D(num_filters, (3, 3), activation='relu', padding='same'))\n",
        "        model.add(layers.MaxPooling2D((2, 2)))\n",
        "\n",
        "    # 3. Flatten features for classification\n",
        "    model.add(layers.Flatten())\n",
        "\n",
        "    # 4. Dense Layer (The brain)\n",
        "    model.add(layers.Dense(128, activation='relu'))\n",
        "\n",
        "    # 5. Optional Dropout (Question 2)\n",
        "    if use_dropout:\n",
        "        model.add(layers.Dropout(0.5))\n",
        "\n",
        "    # 6. Output Layer (One neuron per person)\n",
        "    model.add(layers.Dense(num_classes, activation='softmax'))\n",
        "\n",
        "    model.compile(optimizer='adam',\n",
        "                  loss='sparse_categorical_crossentropy',\n",
        "                  metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "print(\"Step 3 Complete: Model Builder defined.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qLPn4MyOuCox",
        "outputId": "ea72e3b5-7180-4baf-a00d-32705eb03a39"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 3 Complete: Model Builder defined.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Get number of classes\n",
        "classes = [d for d in os.listdir(DATA_PATH) if os.path.isdir(os.path.join(DATA_PATH, d))]\n",
        "num_classes = len(classes)\n",
        "print(f\"Classes found: {num_classes}\")\n",
        "\n",
        "# Load data once for this experiment\n",
        "train_ds, val_ds = load_data(64) # Using 64x64 as a middle ground\n",
        "\n",
        "results_layers = {}\n",
        "\n",
        "print(\"\\n=== EXPERIMENT 1 START ===\")\n",
        "for n in [1, 2, 3]:\n",
        "    print(f\"\\nTraining with {n} Conv Layers...\")\n",
        "    model = build_cnn_model((64, 64, 3), n, use_dropout=False, num_classes=num_classes)\n",
        "\n",
        "    # Train\n",
        "    history = model.fit(train_ds, validation_data=val_ds, epochs=EPOCHS, verbose=0)\n",
        "\n",
        "    # Record Accuracy\n",
        "    acc = history.history['val_accuracy'][-1]\n",
        "    results_layers[n] = acc\n",
        "    print(f\"--> Accuracy: {acc:.4f}\")\n",
        "\n",
        "best_layers = max(results_layers, key=results_layers.get)\n",
        "print(f\"\\nWinner: {best_layers} layers\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_35X33youLT1",
        "outputId": "5ae93c5e-4cfa-4088-80f0-d033c736310f"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Classes found: 5\n",
            "\n",
            "--- Loading Data at size 64x64 ---\n",
            "Found 2626 files belonging to 5 classes.\n",
            "Using 2101 files for training.\n",
            "Found 2626 files belonging to 5 classes.\n",
            "Using 525 files for validation.\n",
            "\n",
            "=== EXPERIMENT 1 START ===\n",
            "\n",
            "Training with 1 Conv Layers...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/layers/core/input_layer.py:27: UserWarning: Argument `input_shape` is deprecated. Use `shape` instead.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--> Accuracy: 0.9943\n",
            "\n",
            "Training with 2 Conv Layers...\n",
            "--> Accuracy: 0.9867\n",
            "\n",
            "Training with 3 Conv Layers...\n",
            "--> Accuracy: 0.9810\n",
            "\n",
            "Winner: 1 layers\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n=== EXPERIMENT 2 START ===\")\n",
        "results_dropout = {}\n",
        "\n",
        "for use_drop in [False, True]:\n",
        "    status = \"WITH Dropout\" if use_drop else \"WITHOUT Dropout\"\n",
        "    print(f\"\\nTraining {status}...\")\n",
        "\n",
        "    model = build_cnn_model((64, 64, 3), best_layers, use_dropout=use_drop, num_classes=num_classes)\n",
        "\n",
        "    history = model.fit(train_ds, validation_data=val_ds, epochs=EPOCHS, verbose=0)\n",
        "    acc = history.history['val_accuracy'][-1]\n",
        "    results_dropout[use_drop] = acc\n",
        "    print(f\"--> Accuracy: {acc:.4f}\")\n",
        "\n",
        "best_dropout = max(results_dropout, key=results_dropout.get)\n",
        "print(f\"\\nWinner: {'Dropout' if best_dropout else 'No Dropout'}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AGkU0IQRuSZp",
        "outputId": "942c4ca2-6c48-4580-ee8b-8c51a63c8e9d"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== EXPERIMENT 2 START ===\n",
            "\n",
            "Training WITHOUT Dropout...\n",
            "--> Accuracy: 0.9867\n",
            "\n",
            "Training WITH Dropout...\n",
            "--> Accuracy: 0.9962\n",
            "\n",
            "Winner: Dropout\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n=== EXPERIMENT 3 START ===\")\n",
        "sizes = [32, 64, 128]\n",
        "\n",
        "for size in sizes:\n",
        "    print(f\"\\nTesting Size: {size}x{size}...\")\n",
        "\n",
        "    # 1. Load Data at new size\n",
        "    t_ds, v_ds = load_data(size)\n",
        "\n",
        "    # 2. Build Model (using best settings found previously)\n",
        "    model = build_cnn_model((size, size, 3), best_layers, best_dropout, num_classes)\n",
        "\n",
        "    # 3. Train and Time\n",
        "    start = time.time()\n",
        "    history = model.fit(t_ds, validation_data=v_ds, epochs=EPOCHS, verbose=0)\n",
        "    end = time.time()\n",
        "\n",
        "    duration = end - start\n",
        "    acc = history.history['val_accuracy'][-1]\n",
        "\n",
        "    print(f\"--> Size {size}x{size} took {duration:.2f} seconds. Accuracy: {acc:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sS3KL2ojvW1A",
        "outputId": "12eb86d7-e01d-4107-a50f-fb545bd63222"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== EXPERIMENT 3 START ===\n",
            "\n",
            "Testing Size: 32x32...\n",
            "\n",
            "--- Loading Data at size 32x32 ---\n",
            "Found 2626 files belonging to 5 classes.\n",
            "Using 2101 files for training.\n",
            "Found 2626 files belonging to 5 classes.\n",
            "Using 525 files for validation.\n",
            "--> Size 32x32 took 14.28 seconds. Accuracy: 0.9905\n",
            "\n",
            "Testing Size: 64x64...\n",
            "\n",
            "--- Loading Data at size 64x64 ---\n",
            "Found 2626 files belonging to 5 classes.\n",
            "Using 2101 files for training.\n",
            "Found 2626 files belonging to 5 classes.\n",
            "Using 525 files for validation.\n",
            "--> Size 64x64 took 43.45 seconds. Accuracy: 0.9886\n",
            "\n",
            "Testing Size: 128x128...\n",
            "\n",
            "--- Loading Data at size 128x128 ---\n",
            "Found 2626 files belonging to 5 classes.\n",
            "Using 2101 files for training.\n",
            "Found 2626 files belonging to 5 classes.\n",
            "Using 525 files for validation.\n",
            "--> Size 128x128 took 195.96 seconds. Accuracy: 0.9924\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ArNUMvKPv4Hy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To build the best Face Recognition model for your dataset:\n",
        "\n",
        "Use 64x64 or 128x128 images (32x32 is too small for faces).\n",
        "\n",
        "Use 2 or 3 Convolutional Layers.\n",
        "\n",
        "Always use Dropout if you notice your training accuracy is much higher than your test accuracy."
      ],
      "metadata": {
        "id": "mZA6DdVWxWv7"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "UxpX6t0LxYKa"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "V5E1"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "TPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}